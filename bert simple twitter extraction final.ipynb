{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0420 16:27:33.393460  1488 file_utils.py:41] PyTorch version 1.2.0+cpu available.\n",
      "I0420 16:27:36.589814  1488 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27482, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', header=None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "             0                                                  1  \\\n",
      "0       textID                                               text   \n",
      "1   cb774db0d1                I`d have responded, if I were going   \n",
      "2   549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "3   088c60f138                          my boss is bullying me...   \n",
      "4   9642c003ef                     what interview! leave me alone   \n",
      "..         ...                                                ...   \n",
      "95  b63ed6da62  eating ice cream and then getting ready for gr...   \n",
      "96  4c41a35a2a        Happy Mothers day to all you Mums out there   \n",
      "97  61a8e11e8a   CASEY`S GONE?!?! BUT WHY?! So, she piddled a ...   \n",
      "98  447dc22c81       hemp cloth is marvelous but unfortunately no   \n",
      "99  3408db03a3  Gonna read a story bout adam lambert online th...   \n",
      "\n",
      "                                                    2          3  \n",
      "0                                       selected_text  sentiment  \n",
      "1                 I`d have responded, if I were going    neutral  \n",
      "2                                            Sooo SAD   negative  \n",
      "3                                         bullying me   negative  \n",
      "4                                      leave me alone   negative  \n",
      "..                                                ...        ...  \n",
      "95  eating ice cream and then getting ready for gr...    neutral  \n",
      "96        Happy Mothers day to all you Mums out there   positive  \n",
      "97                                            freaked   negative  \n",
      "98                                      unfortunately   negative  \n",
      "99  Gonna read a story bout adam lambert online th...    neutral  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df[:100]\n",
    "batch_1 = df\n",
    "print(len(df))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral      43\n",
       "negative     33\n",
       "positive     23\n",
       "sentiment     1\n",
       "Name: 3, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1[3].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = batch_1.drop(0,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "neutral     43\n",
      "negative    33\n",
      "positive    23\n",
      "Name: 3, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(batch_1))\n",
    "print(batch_1[3].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0                                                  1  \\\n",
      "1   cb774db0d1                I`d have responded, if I were going   \n",
      "2   549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "3   088c60f138                          my boss is bullying me...   \n",
      "4   9642c003ef                     what interview! leave me alone   \n",
      "5   358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "..         ...                                                ...   \n",
      "95  b63ed6da62  eating ice cream and then getting ready for gr...   \n",
      "96  4c41a35a2a        Happy Mothers day to all you Mums out there   \n",
      "97  61a8e11e8a   CASEY`S GONE?!?! BUT WHY?! So, she piddled a ...   \n",
      "98  447dc22c81       hemp cloth is marvelous but unfortunately no   \n",
      "99  3408db03a3  Gonna read a story bout adam lambert online th...   \n",
      "\n",
      "                                                    2         3  \n",
      "1                 I`d have responded, if I were going   neutral  \n",
      "2                                            Sooo SAD  negative  \n",
      "3                                         bullying me  negative  \n",
      "4                                      leave me alone  negative  \n",
      "5                                       Sons of ****,  negative  \n",
      "..                                                ...       ...  \n",
      "95  eating ice cream and then getting ready for gr...   neutral  \n",
      "96        Happy Mothers day to all you Mums out there  positive  \n",
      "97                                            freaked  negative  \n",
      "98                                      unfortunately  negative  \n",
      "99  Gonna read a story bout adam lambert online th...   neutral  \n",
      "\n",
      "[99 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0420 16:28:07.476268  1488 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\JAINY\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0420 16:28:09.393769  1488 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\JAINY\\.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.587f67ec28c540f4294c9c2ac7dcf7841ff371aeb12cdeb6a17f69da39ad9452\n",
      "I0420 16:28:09.393769  1488 configuration_utils.py:319] Model config DistilBertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"dim\": 768,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tie_weights_\": true,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0420 16:28:10.984395  1488 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at C:\\Users\\JAINY\\.cache\\torch\\transformers\\7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = batch_1.drop([0,1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "                                                    2         3\n",
      "1                 I`d have responded, if I were going   neutral\n",
      "2                                            Sooo SAD  negative\n",
      "3                                         bullying me  negative\n",
      "4                                      leave me alone  negative\n",
      "5                                       Sons of ****,  negative\n",
      "..                                                ...       ...\n",
      "95  eating ice cream and then getting ready for gr...   neutral\n",
      "96        Happy Mothers day to all you Mums out there  positive\n",
      "97                                            freaked  negative\n",
      "98                                      unfortunately  negative\n",
      "99  Gonna read a story bout adam lambert online th...   neutral\n",
      "\n",
      "[99 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(len(batch_1))\n",
    "print(batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(batch_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(batch_1)):\n",
    "    batch_1[2][i] = str(batch_1[2][i])\n",
    "    #tokenized = tokenizer.encode(batch_1[2][i], add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = batch_1[2].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 39)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 39)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "attention_mask = torch.tensor(attention_mask).to(torch.int64)\n",
    "input_ids = torch.tensor(input_ids).to(torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(last_hidden_states[0][3]))\n",
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(99, 768)\n"
     ]
    }
   ],
   "source": [
    "print(type(features))\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch_1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      neutral\n",
      "1      neutral\n",
      "2     negative\n",
      "3     negative\n",
      "4     negative\n",
      "        ...   \n",
      "95     neutral\n",
      "96    positive\n",
      "97    negative\n",
      "98    negative\n",
      "99     neutral\n",
      "Name: 3, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'neutral', 'positive', 'negative', 'negative',\n",
       "       'negative', 'neutral', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'neutral',\n",
       "       'neutral', 'neutral', 'negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6Test",
   "language": "python",
   "name": "python3.6test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
